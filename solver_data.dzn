S = 4;
% Action space
A = 4;
% Transition bounds
transition_bounds = array4d(1..S, 1..A, 1..S, 1..2, [
  20, 25,
  20, 25,
  20, 25,
  20, 25,

  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25,
  
  20, 25,
  20, 25,
  20, 25,
  20, 25
]);
% Rewards
rewards = array3d(1..S, 1..A, 1..S, [
  1,-1,-1,1,
  1,1,-1,-1,
  1,-1,-1,1,
  -1,-1,1,1,
  1,-1,1,1,
  1,1,-1,1,
  1,1,1,-1,
  1,-1,-1,-1,
  -1,-1,1,1,
  1,-1,-1,1,
  -1,-1,-1,-1,
  -1,1,1,1,
  -1,-1,-1,1,
  1,1,1,-1,
  1,1,-1,-1,
  1,1,-1,1
]);
% Agent policy
policy = array2d(1..S, 1..A, [
  25,25,25,25,
  25,25,25,25,
  25,25,25,25,
  25,25,25,25
]);